{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pattern'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-97df391ef9d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWikipedia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaintext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munittest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pattern'"
     ]
    }
   ],
   "source": [
    "from pattern.web import Wikipedia, plaintext\n",
    "from nltk.util import ngrams\n",
    "from string import punctuation\n",
    "from collections import Counter, defaultdict\n",
    "import unittest\n",
    "from numpy import log\n",
    "import codecs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    intro_punc = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "    s = re.sub('['+re.escape(intro_punc)+']+(?:\\s|$)',' ',s)\n",
    "    s = re.sub('(?:\\s|^)['+re.escape(intro_punc)+']+',' ',s)\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    s = s.lower().strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiParser:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "    def get_articles(self, start, depth, max_count):\n",
    "        article = Wikipedia().article(start)\n",
    "        links = article.links\n",
    "        list_of_strings = []\n",
    "        for link in links:\n",
    "            s = Wikipedia().article(link)\n",
    "            if s is not None:\n",
    "                s = clean_text(plaintext(s.source))\n",
    "                list_of_strings.append(s)\n",
    "        return list_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextStatistics:\n",
    "    def __init__(self, articles):\n",
    "        self.list = articles\n",
    "        pass\n",
    "    \n",
    "    def get_top_3grams(self, n, use_idf=False):\n",
    "        dict_trigrams = {}\n",
    "        new_list = []\n",
    "        sentences = []\n",
    "        \n",
    "        for article in self.list:\n",
    "            new = ''.join([i for i in article if (not i.isdigit())])\n",
    "            sentences += re.split('[\\.?!]', new)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_trigrams = list(trigrams(sentence))\n",
    "            for sentence_trigram in sentence_trigrams:\n",
    "                sentence_trigram_join = ''.join(sentence_trigram)\n",
    "                if sentence_trigram_join in dict_trigrams:\n",
    "                    dict_trigrams[sentence_trigram_join] += 1\n",
    "                else:\n",
    "                    dict_trigrams[sentence_trigram_join] = 1\n",
    "                               \n",
    "        if use_idf == True:\n",
    "            for gram in dict_trigrams:\n",
    "                gram_in_sentences = [gram in i for i in sentences].count(True)\n",
    "                idf = math.log(len(sentences) / float(gram_in_sentences))\n",
    "                dict_trigrams[gram] *= idf\n",
    "        items_sorted = sorted(dict_trigrams.items(), key=lambda pair:(pair[1], pair[0]), reverse=True)\n",
    "        items_sorted_trigrs = [i[0] for i in items_sorted]\n",
    "        items_sorted_freqs = [i[1] for i in items_sorted]\n",
    "        list_of_3grams_in_descending_order_by_freq = items_sorted_trigrs[:n]\n",
    "        list_of_their_corresponding_freq = items_sorted_freqs[:n]\n",
    "        return (list_of_3grams_in_descending_order_by_freq, list_of_their_corresponding_freq)\n",
    "    \n",
    "    def get_top_words(self, n, use_idf=False):\n",
    "        dict_words = {}\n",
    "        pos_articles = set(['a', 'an', 'the'])\n",
    "        some_stop_words = ['and', 'are', 'that', 'to', 'this', 'which']\n",
    "        pos_prepositions = set(['aboard', 'about', 'above', 'across', 'afore', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'around', 'as', 'aside', 'aslant', 'astride', 'at', 'athwart', 'atop', 'between', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'betwixt', 'beyond', 'by', 'circa', 'despite','down', 'except', 'for', 'from', 'in', 'inside', 'into', 'like', 'near', 'neath', 'next', 'of', 'off', 'on', 'opposite', 'out', 'outside', 'over', 'per', 'through', 'till'', toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'with', 'without'])\n",
    "        sentences = []\n",
    "        \n",
    "        for article in self.list:\n",
    "            new = ''.join([i for i in article if not i.isdigit()])\n",
    "            sentences += re.split('[\\.?!]', new)\n",
    "            \n",
    "        for sentence in sentences:\n",
    "            sentence_words = [word for word in sentence.split() if ((not (word in pos_articles)) and (not (word in pos_prepositions)))]\n",
    "\n",
    "            for sentence_word in sentence_words:\n",
    "                if sentence_word in dict_words:\n",
    "                    dict_words[sentence_word] += 1\n",
    "                else:\n",
    "                    dict_words[sentence_word] = 1\n",
    "                    \n",
    "        if use_idf == True:\n",
    "            for word in dict_words:\n",
    "                word_in_sentences = [word in i for i in sentences].count(True)\n",
    "                idf = math.log(len(sentences) / float(word_in_sentences))\n",
    "                dict_words[word] *= idf\n",
    "        items_sorted = sorted(dict_words.items(), key=lambda pair:(pair[1], pair[0]), reverse=True)\n",
    "        items_sorted_words = [i[0] for i in items_sorted]\n",
    "        items_sorted_freqs = [i[1] for i in items_sorted]\n",
    "        list_of_words_in_descending_order_by_freq = items_sorted_words[:n]\n",
    "        list_of_their_corresponding_freq = items_sorted_freqs[:n]\n",
    "        return (list_of_words_in_descending_order_by_freq, list_of_their_corresponding_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def show_results(self):\n",
    "        parser = WikiParser()\n",
    "        articles = parser.get_articles('Natural language processing',1,1)\n",
    "        stats = TextStatistics(articles)\n",
    "        a_stats = (stats.get_top_3grams(20,use_idf=True),\n",
    "                   stats.get_top_words(20,use_idf=True))\n",
    "        res = 'top-20 n-grams for articles from NLP article:\\n%s\\n\\ntop-20 words for articles from NLP article:\\n%s'\n",
    "        print(res % ('\\n'.join([x+' - '+str(y) for x,y in zip(*a_stats[0])]),\n",
    "                     '\\n'.join([x+' - '+str(y) for x,y in zip(*a_stats[1])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_exp = Experiment()\n",
    "try_exp.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
